{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381b8c0f",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "An investment company is looking to collate a list of posts on reddit for investment-related trends, which they can recommend the type of investment to a client based on their comments. Noting that the comments are mostly from retail investors discussing between stocks and cryptocurrency, they want to classify these comments accordingly. The company has commissioned you to build a classification model based off subreddits specifically from r/CryptoCurrency and r/stocks, to help train and build the model to classify these comments to the correct topic. This would allow the the company to suggest the correct type of investments a client should make based on their comments.\n",
    "\n",
    "- Scrape sample comments off subreddits r/CryptoCurrency and r/stocks\n",
    "- To build a classification model that can classify the subreddits with minimal misclassifications "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261feeb",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "303b97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c8941",
   "metadata": {},
   "source": [
    "## Helper methods for Web Scraping off Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7424a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subreddit):\n",
    "    continue_scrape = True\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size' : 100,\n",
    "    }\n",
    "    comb_df = pd.DataFrame()\n",
    "\n",
    "    while continue_scrape:\n",
    "        res = requests.get(base_url,params)\n",
    "        while res.status_code != 200:\n",
    "            time.sleep(0.5)\n",
    "            res = requests.get(base_url,params)\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        tmp_df = pd.DataFrame(posts)\n",
    "\n",
    "        comb_df = pd.concat([comb_df,tmp_df.loc[(tmp_df['selftext'] != '') & (tmp_df['selftext'] != '[removed]') & (tmp_df['selftext'] != '[deleted]') & (pd.notna(tmp_df['selftext'])), :]], ignore_index=True)\n",
    "        #params['before'] = comb_df.loc[comb_df.index[-1],'created_utc']\n",
    "        params['before'] = tmp_df.loc[tmp_df.index[-1],'created_utc']\n",
    "        \n",
    "#         print(comb_df.shape)\n",
    "        if len(comb_df) > 1000:\n",
    "            continue_scrape = False\n",
    "\n",
    "    return comb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b7c79",
   "metadata": {},
   "source": [
    "## Creation of filepath to store scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28ee82c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "outdir = '../data'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e620c25c",
   "metadata": {},
   "source": [
    "## Scraping and storing of files into .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crypto_df = get_data('cryptocurrency')\n",
    "crypto_df.to_csv('../data/cryptocurrency.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac72fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 70)\n",
      "(84, 70)\n",
      "(130, 71)\n",
      "(166, 71)\n",
      "(202, 71)\n",
      "(247, 71)\n",
      "(282, 71)\n",
      "(313, 71)\n",
      "(354, 71)\n",
      "(401, 71)\n",
      "(440, 71)\n",
      "(478, 71)\n",
      "(509, 71)\n",
      "(540, 71)\n",
      "(578, 71)\n",
      "(611, 71)\n",
      "(650, 71)\n",
      "(692, 71)\n",
      "(733, 71)\n",
      "(768, 71)\n",
      "(804, 71)\n",
      "(847, 71)\n",
      "(878, 71)\n",
      "(914, 71)\n",
      "(949, 71)\n",
      "(979, 71)\n",
      "(1017, 71)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stocks_df = get_data('stocks')\n",
    "stocks_df.to_csv('../data/stocks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9459f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
